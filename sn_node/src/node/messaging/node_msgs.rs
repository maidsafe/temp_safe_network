// Copyright 2023 MaidSafe.net limited.
//
// This SAFE Network Software is licensed to you under The General Public License (GPL), version 3.
// Unless required by applicable law or agreed to in writing, the SAFE Network Software distributed
// under the GPL Licence is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied. Please review the Licences for the specific language governing
// permissions and limitations relating to use of the SAFE Network Software.

use crate::{
    node::{
        flow_ctrl::cmds::Cmd, messaging::Recipients, MyNode, NodeContext, RejoinReason, Result,
    },
    storage::{DataStorage, Error as StorageError, StorageLevel},
};
use ed25519_dalek::Keypair as EdKeypair;
use qp2p::{SendStream, UsrMsgBytes};
use sn_comms::Comm;
use sn_fault_detection::IssueType;
use sn_interface::{
    messaging::{
        data::{CmdResponse, DataResponse},
        system::{JoinResponse, NodeEvent, NodeMsg},
        Dst, MsgId, NetworkMsg, WireMsg,
    },
    network_knowledge::{MembershipState, NetworkKnowledge},
    types::{log_markers::LogMarker, ClientId, Keypair, NodeId, PublicKey, ReplicatedData},
    SectionAuthorityProvider,
};
use std::collections::BTreeSet;
use std::sync::Arc;
use xor_name::XorName;

impl MyNode {
    /// Send a (`NetworkMsg`) to nodes
    /// NB: Since we are sending via comms, which only sends to nodes in our section
    /// this is only taking node ids as recipients.
    /// IF we need to send to clients here as well, we'll need to update the fn signature.
    pub(crate) fn send_msg(
        msg: NetworkMsg,
        msg_id: MsgId,
        recipients: BTreeSet<NodeId>,
        our_name: XorName,
        network_knowledge: NetworkKnowledge,
        comm: Comm,
    ) -> Result<()> {
        debug!("Sending msg: {msg_id:?}");
        let msgs = into_msg_bytes(&network_knowledge, our_name, msg, msg_id, recipients)?;

        msgs.into_iter()
            .for_each(|(node_id, msg)| comm.send_out_bytes(node_id, msg_id, msg));

        Ok(())
    }

    /// Send a (`NodeMsg`) message to all Elders in our section
    pub(crate) fn send_to_elders(network_knowledge: &NetworkKnowledge, msg: NodeMsg) -> Cmd {
        let sap = network_knowledge.section_auth();
        let recipients = sap.elders_set();
        Cmd::send_msg(msg, Recipients::Multiple(recipients))
    }

    /// Send a (`NodeMsg`) message to all Elders in our section, await all responses & enqueue
    pub(crate) fn send_to_elders_await_responses(
        sap: SectionAuthorityProvider,
        msg: NodeMsg,
    ) -> Cmd {
        let recipients = sap.elders_set();
        Cmd::SendMsgEnqueueAnyResponse {
            msg,
            msg_id: MsgId::new(),
            recipients,
        }
    }

    #[allow(clippy::too_many_arguments)]
    pub(crate) async fn store_data_and_respond(
        network_knowledge: &NetworkKnowledge,
        ed_keypair: Arc<EdKeypair>,
        data_storage: DataStorage,
        joins_allowed_until_split: bool,
        joins_allowed: bool,
        is_elder: bool,
        data: ReplicatedData,
        send_stream: SendStream,
        client_id: ClientId,
        correlation_id: MsgId,
    ) -> Result<Vec<Cmd>> {
        let mut cmds = vec![];
        let section_pk = PublicKey::Bls(network_knowledge.section_key());

        let node_keypair = Keypair::Ed25519(ed_keypair.clone());

        let data_addr = data.address();

        trace!("About to store data from {correlation_id:?}: {data_addr:?}");

        // This may return a DatabaseFull error... but we should have
        // reported StorageError::NotEnoughSpace well before this
        let response = match data_storage
            .store(&data, section_pk, node_keypair.clone())
            .await
        {
            Ok(storage_level) => {
                info!("{correlation_id:?} Data has been stored: {data_addr:?}");
                if matches!(storage_level, StorageLevel::Updated(_level)) {
                    // we add a new node for every level increase of used space
                    cmds.push(Cmd::SetJoinsAllowed(true));
                } else if data_storage.has_reached_min_capacity() && !joins_allowed_until_split {
                    // we accept new nodes until split, since we have reached the min capacity (i.e. storage limit)
                    cmds.push(Cmd::SetJoinsAllowedUntilSplit(true));
                }
                CmdResponse::ok(data)?
            }
            Err(StorageError::NotEnoughSpace) => {
                // storage full
                error!("Not enough space to store data {data_addr:?}");
                let msg = NodeMsg::NodeEvent(NodeEvent::CouldNotStoreData {
                    node_id: node_keypair.public_key(),
                    data_address: data.address(),
                    full: true,
                });

                if is_elder && !joins_allowed {
                    // we accept new nodes until split, since we ran out of space
                    cmds.push(Cmd::SetJoinsAllowedUntilSplit(true));
                }

                cmds.push(MyNode::send_to_elders(network_knowledge, msg));
                CmdResponse::err(data, StorageError::NotEnoughSpace.into())?
            }
            Err(error) => {
                // the rest seem to be non-problematic errors.. (?)
                // this could be an "we already have it" error... so we should continue with that...
                error!("Problem storing data {data_addr:?}, but it was ignored: {error}");
                CmdResponse::ok(data)?
            }
        };

        let msg = DataResponse::CmdResponse {
            response,
            correlation_id,
        };
        cmds.push(Cmd::send_data_response(
            msg,
            correlation_id,
            client_id,
            send_stream,
        ));

        Ok(cmds)
    }

    // Handler for data messages which have successfully
    // passed all signature checks and msg verifications
    pub(crate) fn handle_node_msg(
        node: &mut MyNode,
        context: NodeContext,
        msg_id: MsgId,
        msg: NodeMsg,
        node_id: NodeId,
        send_stream: Option<SendStream>,
    ) -> Result<Vec<Cmd>> {
        debug!("{:?}: {msg_id:?}", LogMarker::NodeMsgToBeHandled);

        match msg {
            NodeMsg::TryJoin(relocation) => {
                trace!("Handling msg {:?}: TryJoin from {}", msg_id, node_id);
                MyNode::handle_join(node, &context, node_id, msg_id, relocation, send_stream)
                    .map(|c| c.into_iter().collect())
            }
            NodeMsg::PrepareToRelocate(relocation_trigger) => {
                trace!("Handling PrepareToRelocate msg from {node_id}: {msg_id:?}");
                Ok(node.prepare_to_relocate(relocation_trigger))
            }
            NodeMsg::ProceedRelocation(dst) => {
                trace!("Handling ProceedRelocation msg from {node_id}: {msg_id:?}");
                Ok(node.proceed_relocation(node_id.name(), dst)?)
            }
            NodeMsg::CompleteRelocation(signed_relocation) => {
                trace!("Handling CompleteRelocation msg from {node_id}: {msg_id:?}");
                Ok(node.relocate(signed_relocation)?.into_iter().collect())
            }
            // The approval or rejection of a join (approval both for new network joiner as well as
            // existing node relocated to the section) will be received here.
            NodeMsg::JoinResponse(join_response) => {
                if context.network_knowledge.is_section_member(&context.name) {
                    // we can ignore this reponse msg
                    trace!("Join response received when we're already a member. Ignoring.");
                    return Ok(vec![]);
                }

                match join_response {
                    JoinResponse::JoinsDisallowed => {
                        Err(super::Error::RejoinRequired(RejoinReason::JoinsDisallowed))
                    }
                    JoinResponse::Approved(decision) => {
                        info!("{}", LogMarker::ReceivedJoinApproval);
                        let target_sap = context.network_knowledge.signed_sap();

                        if let Err(e) = decision.validate(&target_sap.section_key()) {
                            error!("Failed to validate with {target_sap:?}, dropping invalid join decision: {e:?}");
                            return Ok(vec![]);
                        }

                        // Ensure this decision includes us as a joining node
                        if decision
                            .proposals
                            .keys()
                            .filter(|n| n.state() == MembershipState::Joined)
                            .all(|n| n.name() != context.name)
                        {
                            trace!("MyNode named: {:?} Ignore join approval decision not for us: {decision:?}", context.name);
                            return Ok(vec![]);
                        }

                        info!(
                            "=========>> This node ({:?} @ {:?}) has been approved to join the section at {:?}!", context.name, context.info.addr,
                            target_sap.prefix(),
                        );

                        // If we were relocating finalise it
                        if context.relocation_state.proof().is_some() {
                            node.finalise_relocation(&context, decision);
                        }

                        Ok(vec![])
                    }
                    JoinResponse::UnderConsideration => {
                        info!("Our join request is being considered by the network");
                        Ok(vec![])
                    }
                }
            }
            NodeMsg::HandoverVotes(votes) => node.handle_handover_msg(node_id, votes),
            NodeMsg::HandoverAE(gen) => Ok(node
                .handle_handover_anti_entropy(node_id, gen)
                .into_iter()
                .collect()),
            NodeMsg::MembershipVotes(votes) => {
                let mut cmds = vec![];
                cmds.extend(node.handle_membership_votes(node_id, votes)?);
                Ok(cmds)
            }
            NodeMsg::ProposeNodeOff {
                vote_node_off: proposal,
                sig_share,
            } => {
                if node.is_not_elder() {
                    trace!("Adult handling a ProposeSectionState msg from {node_id}: {msg_id:?}",);
                }

                trace!("Handling ProposeSectionState msg {proposal:?} from {node_id}: {msg_id:?}",);
                node.untrack_node_issue(node_id.name(), IssueType::ElderVoting);
                node.handle_section_state_proposal(msg_id, proposal, sig_share, node_id)
            }
            NodeMsg::DkgStart(session_id, elder_sig) => {
                trace!(
                    "Handling msg: DkgStart s{} {:?}: {} elders from {node_id}",
                    session_id.sh(),
                    session_id.prefix,
                    session_id.elders.len(),
                );

                node.untrack_node_issue(node_id.name(), IssueType::Dkg);
                node.handle_dkg_start(session_id, elder_sig)
            }
            NodeMsg::DkgEphemeralPubKey {
                session_id,
                section_auth,
                pub_key,
                sig,
            } => {
                trace!(
                    "{} s{} from {node_id}",
                    LogMarker::DkgHandleEphemeralPubKey,
                    session_id.sh(),
                );
                node.handle_dkg_ephemeral_pubkey(&session_id, section_auth, pub_key, sig, node_id)
            }
            NodeMsg::DkgVotes {
                session_id,
                pub_keys,
                votes,
            } => {
                trace!(
                    "{} s{} from {node_id}: {votes:?}",
                    LogMarker::DkgVotesHandling,
                    session_id.sh(),
                );
                node.untrack_node_issue(node_id.name(), IssueType::Dkg);

                node.handle_dkg_votes(&session_id, pub_keys, votes, node_id)
            }

            NodeMsg::NodeEvent(NodeEvent::CouldNotStoreData {
                node_id,
                data_address,
                full,
            }) => {
                info!("Processing CouldNotStoreData event with {msg_id:?} at: {data_address:?}, (node reporting full: {full})");

                if !context.is_elder {
                    error!("Received unexpected message while Adult");
                    return Ok(vec![]);
                }

                let mut cmds = vec![];

                if !context.joins_allowed {
                    cmds.push(Cmd::SetJoinsAllowed(true));
                    // NB: we do not also set allowed until split, since we
                    // do not expect another node to run out of space before we ourselves
                    // have reached the storage limit (i.e. the `min_capacity` variable, which
                    // should be set by the node operator to be a little bit lower than the actual space).
                }

                // only when the node is severely out of sync with the rest, do we vote it off straight away
                // othwerwise we report it as an issue (see further down)
                if full && context.data_storage.is_below_half_limit() {
                    warn!("Node {node_id} prematurely reported full. Voting it off..");
                    let nodes = BTreeSet::from([node_id.into()]);
                    cmds.push(Cmd::ProposeVoteNodesOffline(nodes));
                    return Ok(cmds);
                }

                // we report it as an issue, to give it some slack
                context.track_node_issue(node_id.into(), IssueType::Communication);

                Ok(cmds)
            }
            NodeMsg::RequestHandover { sap, sig_share } => {
                info!("RequestHandover with msg_id {msg_id:?}");
                node.handle_handover_request(msg_id, sap, sig_share, node_id)
            }
            NodeMsg::SectionHandoverPromotion { sap, sig_share } => {
                info!("SectionHandoverPromotion with msg_id {msg_id:?}");
                node.handle_handover_promotion(msg_id, sap, sig_share, node_id)
            }
            NodeMsg::SectionSplitPromotion {
                sap0,
                sig_share0,
                sap1,
                sig_share1,
            } => {
                info!("SectionSplitPromotion with msg_id {msg_id:?}");
                node.handle_section_split_promotion(
                    msg_id, sap0, sig_share0, sap1, sig_share1, node_id,
                )
            }
            msg => {
                error!("This node msg should have been handled in the non-blocking flow ctrl cmd thread(s): {msg:?}");

                Ok(vec![])
            }
        }
    }
}

// Serializes the msg, producing one [`WireMsg`] instance
// per recipient - the last step before passing it over to comms module.
pub(crate) fn into_msg_bytes(
    network_knowledge: &NetworkKnowledge,
    our_node_name: XorName,
    msg: NetworkMsg,
    msg_id: MsgId,
    recipients: BTreeSet<NodeId>,
) -> Result<Vec<(NodeId, UsrMsgBytes)>> {
    let (kind, payload) = MyNode::serialize_msg(our_node_name, &msg)?;

    // we first generate the XorName
    let dst = Dst {
        name: xor_name::rand::random(),
        section_key: bls::SecretKey::random().public_key(),
    };

    let mut initial_wire_msg = WireMsg::new_msg(msg_id, payload, kind, dst);
    let _bytes = initial_wire_msg.serialize_and_cache_bytes()?;

    let mut msgs = vec![];
    for node_id in recipients {
        match network_knowledge.generate_dst(&node_id.name()) {
            Ok(dst) => {
                debug!("Dst generated for outgoing msg is: {dst:?}");
                // TODO log error here isntead of throwing
                let all_the_bytes = initial_wire_msg.serialize_with_new_dst(&dst)?;
                msgs.push((node_id, all_the_bytes));
            }
            Err(error) => {
                error!("Could not get route for {node_id:?}: {error}");
            }
        }
    }

    Ok(msgs)
}
